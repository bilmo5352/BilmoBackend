#!/usr/bin/env python3
"""
Real Product Search & MongoDB Storage
Uses actual scrapers for Amazon, Flipkart, Meesho, and Myntra
Usage: python real_search_products.py "iphone 15"
"""

import sys
import json
import time
from datetime import datetime
from typing import Dict, List, Optional
import pymongo
from pymongo import MongoClient
from concurrent.futures import ThreadPoolExecutor, as_completed

# Import real scrapers
try:
    from amazon_search import search_amazon
    from flipkart_search import search_flipkart
    from meesho_search import search_meesho
    from myntra_search import search_myntra
except ImportError as e:
    print(f"‚ùå Error importing scrapers: {e}")
    print("Make sure all scraper files are in the current directory")
    sys.exit(1)

# MongoDB Configuration
def connect_mongodb():
    """Connect to MongoDB Atlas"""
    try:
        # Using URL encoding for password with @ symbol
        import urllib.parse
        username = "hrithick"
        password = "hrimee@0514"
        encoded_password = urllib.parse.quote_plus(password)
        uri = f"mongodb+srv://{username}:{encoded_password}@bilmo.jmeclfh.mongodb.net/?retryWrites=true&w=majority&appName=bilmo"
        
        client = MongoClient(uri)
        db = client["scraper_db"]
        collection = db["search_results"]
        # Test connection
        client.admin.command('ping')
        print("‚úÖ Connected to MongoDB")
        return client, db, collection
    except Exception as e:
        print(f"‚ùå MongoDB connection error: {e}")
        print("üí° Continuing without MongoDB - results will be saved to JSON file only")
        return None, None, None

def save_to_mongodb(collection, data, query):
    """Save search results to MongoDB"""
    if not collection:
        return False
        
    try:
        document = {
            "query": query,
            "timestamp": datetime.now(),
            "data": data,
            "total_results": len(data.get("all_products", [])) if isinstance(data, dict) else len(data) if isinstance(data, list) else 1
        }
        
        result = collection.insert_one(document)
        print(f"‚úÖ Saved search results to MongoDB with ID: {result.inserted_id}")
        return True
    except Exception as e:
        print(f"‚ùå Failed to save to MongoDB: {e}")
        return False

def search_single_website(website: str, query: str, headless: bool = True) -> Dict:
    """Search a single website and return results"""
    print(f"üîç Starting {website} search for: {query}")
    start_time = time.time()
    
    try:
        # Run the scraper (they save to files but don't return data)
        if website == "amazon":
            search_amazon(query, headless=headless, max_results=8)
        elif website == "flipkart":
            search_flipkart(query, headless=headless, max_results=8)
        elif website == "meesho":
            search_meesho(query, headless=headless)
        elif website == "myntra":
            search_myntra(query, headless=headless, max_results=8)
        else:
            raise Exception(f"Unknown website: {website}")
        
        # Read the generated JSON files
        result = read_scraper_output(website, query)
        
        end_time = time.time()
        print(f"‚úÖ {website} search completed in {end_time - start_time:.2f}s")
        
        return {
            "site": website,
            "query": query,
            "status": "success",
            "search_time": end_time - start_time,
            "data": result
        }
        
    except Exception as e:
        end_time = time.time()
        print(f"‚ùå {website} search failed: {str(e)}")
        return {
            "site": website,
            "query": query,
            "status": "error",
            "search_time": end_time - start_time,
            "error": str(e),
            "data": None
        }

def read_scraper_output(website: str, query: str) -> Dict:
    """Read the JSON output files generated by scrapers"""
    query_formatted = query.replace(' ', '_')
    
    # Try different possible filenames
    possible_files = [
        f"{website}_detailed_products_{query_formatted}.json",
        f"{website}_products_{query_formatted}.json",
        f"{website}_search_{query_formatted}.json"
    ]
    
    for filename in possible_files:
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                data = json.load(f)
                print(f"‚úÖ Read {website} data from {filename}")
                return data
        except FileNotFoundError:
            continue
        except json.JSONDecodeError as e:
            print(f"‚ö†Ô∏è JSON decode error in {filename}: {e}")
            continue
    
    print(f"‚ö†Ô∏è No data file found for {website}")
    return {"error": f"No data file found for {website}", "products": []}

def extract_products_from_result(website_result):
    """Extract products from a website result"""
    products = []
    if not website_result.get("data") or website_result["status"] != "success":
        return products
    
    data = website_result["data"]
    site = website_result["site"]
    
    # Handle different data structures from different scrapers
    if isinstance(data, dict):
        if "products" in data:
            products = data["products"]
        elif "basic_products" in data:
            products = data["basic_products"]
        elif "detailed_products" in data:
            products = data["detailed_products"]
    elif isinstance(data, list):
        products = data
    
    # Add site information to each product
    for product in products:
        if isinstance(product, dict):
            product["source_website"] = site
    
    return products

def unified_search(query: str, headless: bool = True) -> Dict:
    """Search all websites and combine results"""
    print(f"üöÄ Starting unified search for: '{query}'")
    print("=" * 60)
    
    websites = ["amazon", "flipkart", "meesho", "myntra"]
    results = []
    
    # Use ThreadPoolExecutor for parallel execution
    with ThreadPoolExecutor(max_workers=4) as executor:
        # Submit all search tasks
        future_to_website = {
            executor.submit(search_single_website, website, query, headless): website 
            for website in websites
        }
        
        # Collect results as they complete
        for future in as_completed(future_to_website):
            website = future_to_website[future]
            try:
                result = future.result()
                results.append(result)
            except Exception as e:
                results.append({
                    "site": website,
                    "query": query,
                    "status": "error",
                    "error": str(e),
                    "data": None
                })
    
    # Sort results by website name for consistency
    results.sort(key=lambda x: x["site"])
    
    # Extract all products
    all_products = []
    successful_searches = 0
    total_search_time = 0
    
    for result in results:
        if result["status"] == "success":
            successful_searches += 1
            products = extract_products_from_result(result)
            all_products.extend(products)
        
        total_search_time += result.get("search_time", 0)
    
    # Calculate summary statistics
    prices = []
    brands = set()
    categories = set()
    
    for product in all_products:
        # Extract price for range calculation
        if "price" in product and product["price"]:
            try:
                price_str = str(product["price"]).replace("‚Çπ", "").replace("Rs.", "").replace(",", "").strip()
                # Handle price ranges like "‚Çπ25,999 - ‚Çπ30,999"
                if "-" in price_str:
                    price_str = price_str.split("-")[0].strip()
                if price_str.replace(".", "").isdigit():
                    prices.append(float(price_str))
            except:
                pass
        
        # Extract brand
        if "brand" in product and product["brand"]:
            brands.add(str(product["brand"]).strip())
        
        # Extract category
        if "category" in product and product["category"]:
            categories.add(str(product["category"]).strip())
    
    # Create combined result
    combined_result = {
        "success": True,
        "query": query,
        "search_timestamp": datetime.now().isoformat(),
        "total_results": len(all_products),
        "total_websites": len(websites),
        "successful_searches": successful_searches,
        "failed_searches": len(websites) - successful_searches,
        "total_search_time": round(total_search_time, 2),
        "results": results,
        "all_products": all_products,
        "summary": {
            "total_products": len(all_products),
            "websites_with_data": [r["site"] for r in results if r["status"] == "success" and extract_products_from_result(r)],
            "price_range": {
                "min": min(prices) if prices else None,
                "max": max(prices) if prices else None,
                "count": len(prices)
            },
            "brands_found": sorted(list(brands)),
            "categories_found": sorted(list(categories))
        }
    }
    
    return combined_result

def save_json_file(data, query):
    """Save results to JSON file"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"unified_search_{query.replace(' ', '_')}_{timestamp}.json"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        print(f"üíæ Saved JSON file: {filename}")
        return filename
    except Exception as e:
        print(f"‚ùå Failed to save JSON file: {e}")
        return None

def display_summary(result):
    """Display search results summary"""
    print("\n" + "=" * 60)
    print("üìä SEARCH RESULTS SUMMARY")
    print("=" * 60)
    
    print(f"Query: {result['query']}")
    print(f"Total Websites: {result['total_websites']}")
    print(f"Successful Searches: {result['successful_searches']}")
    print(f"Failed Searches: {result['failed_searches']}")
    print(f"Total Products Found: {result['summary']['total_products']}")
    print(f"Websites with Data: {', '.join(result['summary']['websites_with_data'])}")
    print(f"Total Search Time: {result['total_search_time']} seconds")
    
    if result['summary']['price_range']['min']:
        print(f"Price Range: ‚Çπ{result['summary']['price_range']['min']:.0f} - ‚Çπ{result['summary']['price_range']['max']:.0f} ({result['summary']['price_range']['count']} products)")
    
    if result['summary']['brands_found']:
        brands_display = ', '.join(result['summary']['brands_found'][:5])
        if len(result['summary']['brands_found']) > 5:
            brands_display += f"... (+{len(result['summary']['brands_found']) - 5} more)"
        print(f"Brands Found: {brands_display}")
    
    if result['summary']['categories_found']:
        categories_display = ', '.join(result['summary']['categories_found'][:3])
        if len(result['summary']['categories_found']) > 3:
            categories_display += f"... (+{len(result['summary']['categories_found']) - 3} more)"
        print(f"Categories Found: {categories_display}")

def display_sample_products(result):
    """Display sample products from search results"""
    if not result['all_products']:
        return
        
    print(f"\nüìã SAMPLE PRODUCTS (showing first 5):")
    print("-" * 60)
    
    for i, product in enumerate(result['all_products'][:5], 1):
        print(f"\n{i}. {product.get('name', 'Unknown Product')[:80]}...")
        print(f"   Website: {product.get('source_website', 'Unknown').upper()}")
        print(f"   Price: {product.get('price', 'Not available')}")
        if product.get('brand'):
            print(f"   Brand: {product['brand']}")
        if product.get('rating'):
            print(f"   Rating: {product['rating']}")
        if product.get('link'):
            print(f"   Link: {product['link'][:60]}...")

def main():
    """Main function"""
    if len(sys.argv) < 2:
        query = input("Enter product to search: ").strip()
    else:
        query = " ".join(sys.argv[1:])
    
    if not query:
        print("‚ùå No query provided. Exiting.")
        sys.exit(1)
    
    # Check for headless flag
    headless = "--headless" in sys.argv
    if headless:
        query = query.replace("--headless", "").strip()
    
    print(f"üîç Searching for: '{query}'")
    print(f"üåê Websites: Amazon, Flipkart, Meesho, Myntra")
    print(f"‚öôÔ∏è Headless mode: {headless}")
    print()
    
    # Run search
    start_time = time.time()
    result = unified_search(query, headless=headless)
    end_time = time.time()
    
    # Display summary
    display_summary(result)
    
    # Connect to MongoDB and save
    client, db, collection = connect_mongodb()
    if collection:
        save_to_mongodb(collection, result, query)
        if client:
            client.close()
    
    # Save to JSON file
    json_file = save_json_file(result, query)
    
    # Display sample products
    display_sample_products(result)
    
    print(f"\nüéâ Search completed in {end_time - start_time:.2f} seconds!")
    if json_file:
        print(f"üìÑ JSON file saved: {json_file}")
    print(f"üíæ Results processed for MongoDB storage")

if __name__ == "__main__":
    main()
